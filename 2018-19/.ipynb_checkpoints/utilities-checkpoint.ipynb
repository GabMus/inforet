{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "A collection of utility classes used mainly for accessing different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boardgames query relevance dataset\n",
    "Query relevance in boardgames is calculated using the MongoDB text search functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Games(object):\n",
    "    \n",
    "    def __init__(self, dbname, collection, queries):\n",
    "        self.db = pymongo.MongoClient()[dbname][collection]\n",
    "        self.queries = queries\n",
    "        self.queryfile = self.queries\n",
    "        \n",
    "    def relevance(self, min_relevance=0.0):\n",
    "        data = {'query': [], 'doc': [], 'relevance': []}\n",
    "        for i, query in enumerate(self.queries):\n",
    "            cursor = self.db.find({'$text': {'$search': query}}, \n",
    "                                  {'_id': 1, 'title': 1, \n",
    "                                   'score': {'$meta': 'textScore'}})\n",
    "            for record in cursor:\n",
    "                if record['score'] >= min_relevance:\n",
    "                    data['query'].append(i)\n",
    "                    data['doc'].append(record['_id'])\n",
    "                    data['relevance'].append(record['score'])\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def readdocs(self, file=None):\n",
    "        if file is None:\n",
    "            for record in self.db.find():\n",
    "                yield {'id': record['_id'], 'text': record['description']}\n",
    "        else:\n",
    "            for i, q in enumerate(file):\n",
    "                yield {'id': i, 'text': q.replace('\"', '')}\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for doc in self.readdocs():\n",
    "            yield (doc['id'], doc['text'])\n",
    "    \n",
    "    def count(self):\n",
    "        return self.db.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cranfield 1400 query relevance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cranfield(object):\n",
    "    \n",
    "    def __init__(self, folder, textfields=['text']):\n",
    "        self.textfields = textfields\n",
    "        self.folder = folder\n",
    "        self.docfile = os.sep.join([self.folder, \n",
    "                                    'cran.all.1400'])\n",
    "        self.queryfile = os.sep.join([self.folder, \n",
    "                                      'cran.qry'])\n",
    "        self.relevancefile = os.sep.join([self.folder, \n",
    "                                          'cranqrel'])\n",
    "    \n",
    "    def readdocs(self, file=None):\n",
    "        if file is None:\n",
    "            file = self.docfile\n",
    "        with open(file, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        docs, doc, current = [], None, None\n",
    "        for line in lines:\n",
    "            if line.startswith('.I'):\n",
    "                if doc is None:\n",
    "                    doc = {}\n",
    "                else:\n",
    "                    yield doc\n",
    "                    doc = {}\n",
    "                doc['id'] = int(line.rstrip().split()[-1])\n",
    "            elif line.startswith('.T'):\n",
    "                doc['title'] = \"\"\n",
    "                current = 'title'\n",
    "            elif line.startswith('.A'):\n",
    "                doc['author'] = \"\"\n",
    "                current = 'author'\n",
    "            elif line.startswith('.B'):\n",
    "                doc['venue'] = \"\"\n",
    "                current = 'venue'\n",
    "            elif line.startswith('.W'):\n",
    "                doc['text'] = \"\"\n",
    "                current = 'text'\n",
    "            else:\n",
    "                doc[current] += \" \" + line.strip()\n",
    "        yield doc\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for doc in self.readdocs():\n",
    "            text = \" \".join([doc[field] for \n",
    "                            field in self.textfields])\n",
    "            yield (doc['id'], text)\n",
    "            \n",
    "    def count(self):\n",
    "        return len(list(self.readdocs()))\n",
    "    \n",
    "    def relevance(self):\n",
    "        data = {'query': [], 'doc': [], 'relevance': []}\n",
    "        with open(self.relevancefile, 'r') as rf:\n",
    "            lines = rf.readlines()\n",
    "        for line in lines:\n",
    "            numbers = [int(x) for x in line.strip().split()]\n",
    "            data['query'].append(numbers[0])\n",
    "            data['doc'].append(numbers[1])\n",
    "            data['relevance'].append(numbers[2])\n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK and Spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, language):\n",
    "        try:\n",
    "            self.iso, self.extended = language\n",
    "        except:\n",
    "            print('Error: language should be provided in the form (iso-2, extended)')\n",
    "        self.stemmer = SnowballStemmer(self.extended)\n",
    "        self.nlp = spacy.load(self.iso)\n",
    "        self.keys = ['document', 'sentence', 'position', 'text', 'lower', 'lemma', \n",
    "                        'pos', 'tag', 'dep',\n",
    "                        'shape', 'is_alpha', 'is_stop', 'stem']\n",
    "    \n",
    "    def tokenize(self, text_id, text, drop_apostrophe=False):\n",
    "        if drop_apostrophe:\n",
    "            text = text.replace(\"'\", \" \")\n",
    "        tokens = []\n",
    "        for j, sentence in enumerate(sent_tokenize(text)):\n",
    "            doc = self.nlp(sentence.strip())\n",
    "            for i, token in enumerate(doc):\n",
    "                lower = token.text.lower()\n",
    "                tag_data = [tuple(x.split('=')) for x in \n",
    "                            token.tag_.split('|')]\n",
    "                try:\n",
    "                    tag = dict(tag_data)\n",
    "                except ValueError:\n",
    "                    tag = tag_data[0][0]\n",
    "                data = [text_id, j, i, token.text, lower, token.lemma_, \n",
    "                        token.pos_, tag, token.dep_,\n",
    "                        token.shape_, token.is_alpha, token.is_stop, self.stemmer.stem(lower)]\n",
    "                tokens.append(dict(zip(self.keys, data)))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
