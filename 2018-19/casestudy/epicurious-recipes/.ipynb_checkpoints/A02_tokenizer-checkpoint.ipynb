{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK and Spacy tokenizers for recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, language):\n",
    "        try:\n",
    "            self.iso, self.extended = language\n",
    "        except:\n",
    "            print('Error: language should be provided in the form (iso-2, extended)')\n",
    "        self.stemmer = SnowballStemmer(self.extended)\n",
    "        self.nlp = spacy.load(self.iso)\n",
    "        self.keys = ['document', 'sentence', 'position', 'text', 'lower', 'lemma', \n",
    "                        'pos', 'tag', 'dep',\n",
    "                        'shape', 'is_alpha', 'is_stop', 'stem']\n",
    "    \n",
    "    def tokenize(self, text_id, text, text_label='full', drop_apostrophe=False):\n",
    "        if drop_apostrophe:\n",
    "            text = text.replace(\"'\", \" \")\n",
    "        tokens = []\n",
    "        for j, sentence in enumerate(sent_tokenize(text)):\n",
    "            doc = self.nlp(sentence.strip())\n",
    "            for i, token in enumerate(doc):\n",
    "                lower = token.text.lower()\n",
    "                tag_data = [tuple(x.split('=')) for x in \n",
    "                            token.tag_.split('|')]\n",
    "                try:\n",
    "                    tag = dict(tag_data)\n",
    "                except ValueError:\n",
    "                    tag = tag_data[0][0]\n",
    "                data = [text_id, j, i, token.text, lower, token.lemma_, \n",
    "                        token.pos_, tag, token.dep_,\n",
    "                        token.shape_, token.is_alpha, token.is_stop, self.stemmer.stem(lower)]\n",
    "                record = dict(zip(self.keys, data))\n",
    "                record['label'] = text_label\n",
    "                tokens.append(record)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Store output of tokenization in the DB working on recepies text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['inforet']\n",
    "source = db['epicurious']\n",
    "target = db['epicurious_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(('en', 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    all_tokens = []\n",
    "    labels = ['title', 'directions', 'ingredients', 'desc']\n",
    "    for label in labels:\n",
    "        if label in record.keys():\n",
    "            if isinstance(record[label], list):\n",
    "                for text in record[label]:\n",
    "                    tokens = tokenizer.tokenize(record['_id'], text, text_label=label)\n",
    "                    all_tokens += tokens\n",
    "            elif record[label] is not None:\n",
    "                tokens = tokenizer.tokenize(record['_id'], record[label], text_label=label)\n",
    "                all_tokens += tokens\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130 of 20130\n"
     ]
    }
   ],
   "source": [
    "N = source.count_documents({})\n",
    "for x in range(0, 22):\n",
    "    limit, entries, ids = x * 1000, [], []\n",
    "    for i, recipe in enumerate(source.find().skip(limit).limit(1000)):\n",
    "        ids.append(recipe['_id'])\n",
    "        entries += tokenize(recipe)\n",
    "        print(i+limit+1, 'of', N)\n",
    "        clear_output(wait=True)\n",
    "    if len(entries) > 0:\n",
    "        target.insert_many(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
