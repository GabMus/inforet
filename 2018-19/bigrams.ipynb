{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval of compund terms\n",
    "Compound terms are n-tuple (2-tuple, 3-tuple, etc.) of terms that can be considered as a single term for linguistic purposed. Examples are New York, Los Angeles, machine learning.\n",
    "\n",
    "The idea of retrieving compund terms is to evaluate the relevance of a sequence of $n$ terms in a corpus by comparing it with the relevance of its term components. We call a sequence of $n$ terms a **n-gram**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information\n",
    "Given two discrete random variables $A$ and $B$, we define their mutual information $M(A,B)$ as:\n",
    "\n",
    "$$\n",
    "    M(A, B) = \\sum\\limits_{a \\in A}\\sum\\limits_{b \\in B} P(a, b) \\log \\left(\\frac{P(a, b)}{P(a)P(b)}\\right)\n",
    "$$\n",
    "\n",
    "The intuition is that we are going to compare the joint distribution of $A$ and $B$ with the assumption that they are independent.\n",
    "\n",
    "When we observe a specific pair of data $a$ and $b$, such as two terms, we can work with the notion of **pointwise mutual information**, defined as:\n",
    "\n",
    "$$\n",
    "PMI(a, B) = \\log \\frac{P(a, b)}{P(a)P(b)},\n",
    "$$\n",
    "where $P(a, b)$ is the observed probability of randomly exctact the pair of terms $(a,b)$ given a corpus $C$ and $P(a)P(b)$ is the expected probability of $(a,b)$ under the assumption that $a$ and $b$ are independent.\n",
    "\n",
    "In a text, we can in general estimate the probability of a term by means of the **maximum likelihood estimation (MLE)** method, by simply comparing the term frequency of the term with the term frequencies of all the terms in the dictionary $D$.\n",
    "\n",
    "$$\n",
    "P(a) = \\frac{count(a)}{\\sum\\limits_{i \\in D}count(i)}\n",
    "$$\n",
    "\n",
    "The same can be done for **2-grams** in order to estimate $P(a, b)$ (or higher order grams for estimating the probability of longer sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cran = pymongo.MongoClient()['inforet']['cran_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequencies(collection, field='text'):\n",
    "    g = {'$group': {'_id': '$' + field, 'count': {'$sum': 1}}}\n",
    "    cursor = collection.aggregate([g])\n",
    "    term_frequencies = dict([(x['_id'], x['count']) for x in cursor])\n",
    "    return term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = term_frequencies(cran, field='text')\n",
    "N = sum(TF.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 18848 0.07802099546312548\n",
      "of 12316 0.0509818856177766\n",
      ". 9761 0.04040550385799914\n",
      ", 6801 0.02815263105606517\n",
      "and 5967 0.024700301354439184\n",
      "a 5686 0.023537106335066397\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(TF.items(), key=lambda x: -x[1])[:6]:\n",
    "    print(k, v, v / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 grams indexing\n",
    "In order to estimate probabilities also for 2-grams, we need a special index of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sentences(collection, field='text'):\n",
    "    s = {'$sort': {'document': 1, 'sentence': 1, 'position': 1}}\n",
    "    g = {'$group': {'_id': {'doc': '$document', 'sent': '$sentence'}, \n",
    "                    'tokens': {'$push': '$'+field}}}\n",
    "    cursor = collection.aggregate([s, g])\n",
    "    sentences = []\n",
    "    for record in cursor:\n",
    "        sentences.append(['#start'] + record['tokens'] + ['#stop'])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_sent = bigram_sentences(cran, field='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "Nb = 0\n",
    "for sent in bi_sent:\n",
    "    for a, b in nltk.ngrams(sent, n=2):\n",
    "        Nb += 1\n",
    "        B[a][b] += 1\n",
    "TF['#start'] = len(bi_sent)\n",
    "TF['#stop'] = TF['#start']\n",
    "N += 2*TF['#start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = {}\n",
    "for k, v in B.items():\n",
    "    Nk = TF[k]\n",
    "    for s, c in v.items():\n",
    "        Ns = TF[s]\n",
    "        M[(k, s)] = (c / Nb) / ((Nk / N) * (Ns / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indium blue 271004.3139046648 1 1\n",
      "fort halstead 271004.3139046648 1 1\n",
      "17,500 meters 271004.3139046648 1 1\n",
      "8000 calibers 271004.3139046648 1 1\n",
      "embryonic lobes 271004.3139046648 1 1\n",
      "decimal digits 271004.3139046648 1 1\n",
      "/length changes/ 271004.3139046648 1 1\n",
      "haveg rocketon 271004.3139046648 1 1\n",
      "rewriting superscript 271004.3139046648 1 1\n",
      "russian investigator 271004.3139046648 1 1\n"
     ]
    }
   ],
   "source": [
    "selected_bigrams = [(x[0], x[1], y) for x, y in \n",
    "                    M.items() if TF[x[0]] > 0 and TF[x[1]] > 0]\n",
    "for k, v, w in sorted(selected_bigrams, key=lambda x: -x[2])[:10]:   \n",
    "    print(k, v, w, TF[k], TF[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another method for estimating 2-grams relevance\n",
    "We can also compute:\n",
    "\n",
    "$$\n",
    "P(b \\mid a) = \\frac{count(a, b)}{\\sum\\limits_{i \\in D} count(a, i)} = \\frac{count(a, b)}{count(a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = {}\n",
    "for k, v in B.items():\n",
    "    Nk = TF[k]\n",
    "    for s, c in v.items():\n",
    "        K[(k, s)] = c / Nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non - 1.0 82 4632\n",
      "semi - 1.0 42 4632\n",
      "proportional to 1.0 23 4437\n",
      "vicinity of 1.0 29 12316\n",
      "navier - 1.0 24 4632\n",
      "subjected to 0.9866666666666667 75 4437\n",
      ". #stop 0.9857596557729741 9761 9685\n",
      "re - 0.9795918367346939 49 4632\n",
      "however , 0.9743589743589743 117 6801\n",
      "presence of 0.9736842105263158 76 12316\n"
     ]
    }
   ],
   "source": [
    "selected_bigrams = [(x[0], x[1], y) for x, y in K.items() if TF[x[0]] > 20 and TF[x[1]] > 20]\n",
    "for k, v, w in sorted(selected_bigrams, key=lambda x: -x[2])[:10]:   \n",
    "    print(k, v, w, TF[k], TF[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that\n",
    "\n",
    "$$\n",
    "\\frac{P(a, b)}{P(a)P(b)} \\approx \\frac{P(b \\mid a)}{P(b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non - 1.0 56.34 58.51\n",
      "semi - 1.0 56.34 58.51\n",
      "proportional to 1.0 58.81 61.08\n",
      "vicinity of 1.0 21.19 22.0\n",
      "navier - 1.0 56.34 58.51\n",
      "subjected to 0.9866666666666667 58.03 60.26\n",
      ". #stop 0.9857596557729741 26.56 27.58\n",
      "re - 0.9795918367346939 55.19 57.31\n",
      "however , 0.9743589743589743 37.38 38.83\n",
      "presence of 0.9736842105263158 20.63 21.43\n"
     ]
    }
   ],
   "source": [
    "selected_bigrams = [(x[0], x[1], y) for x, y in K.items() if TF[x[0]] > 20 and TF[x[1]] > 20]\n",
    "for k, v, w in sorted(selected_bigrams, key=lambda x: -x[2])[:10]:   \n",
    "    print(k, v, w, round(w / (TF[v] / N), 2), round(M[(k, v)], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
